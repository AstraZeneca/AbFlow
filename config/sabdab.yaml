---

# wandb settings
project_name: flow_matching
model_name: all_atom_de_novo

# training settings
trainer:
    devices: 1
    max_epochs: 200
    precision: 32-true # please train the model with 32-bit precision as rotation flow is not stable with 16-bit precision
    val_check_interval: 0.5
    gradient_clip_val: 1.0
checkpoint:
    path: null
    load_optimizer: False

# abflow datamodule settings
datamodule:
    num_workers: 8
    batch_size: 4 # batch size per GPU
    redesign:
        framework: False
        hcdr1: False
        hcdr2: False
        hcdr3: True
        lcdr1: True
        lcdr2: False
        lcdr3: False
    crop: 
        max_crop_size: 180 # ideally 192
        antigen_crop_size: 60 # ideally 64
    dataset:
        name: sabdab
        seed: 2025
        load_val_test: True  # True only during training, and False during data pre-processing
        scheme: chothia       # Scheme to use for sequence alignment. Default: chothia
        num_val_cluster: 10
        paths:
            data: /scratch/hz362/datavol/data/
            model: /scratch/hz362/datavol/model/

# abflow model settings
model:
    loss_weighting:
        sequence_vf_loss: 1.0
        translation_vf_loss: 1.0
        rotation_vf_loss: 1.0
        dihedral_vf_loss: 1.0
        distogram_loss: 1.0
        confidence_lddt_loss: 1.0
    learning_rate: 0.0001
    design_mode: ["sequence", "backbone", "sidechain"] # ["sequence", "backbone", "sidechain"]
    seed: 42

# abflow network settings
network:
    design_mode: ["sequence", "backbone", "sidechain"] # ["sequence", "backbone", "sidechain"]
    c_s: 96 # default: 384
    c_z: 32 # default: 128
    n_condition_module_blocks: 8 # default: 48
    n_denoising_module_blocks: 4 # default: 24
    n_confidence_module_blocks: 1 # default: 4
    n_cycle: 4
    mini_rollout_steps: 20
    full_rollout_steps: 500
    self_condition_rate: 0.0 # not implemented
    self_condition_steps: 0 # not implemented
    label_smoothing: 0.0




# additional parameters which is currently not used - consider removing or adding them
network_params: # hidden dim is c_hidden * N_head. The principle is to keep hidden dim same as feature dim c_s or c_z
    Pairformer:
        TriangleMultiplicationOutgoing:
            c_hidden: 32 # 128
        TriangleMultiplicationIncoming:
            c_hidden: 32 # 128
        TriangleAttentionStartingNode:
            c_hidden: 8 # 32
            N_head: 4
        TriangleAttentionEndingNode:
            c_hidden: 8 # 32
            N_head: 4
        AttentionPairBias:
            c_hidden: 16 # 24
            N_head: 6 # 16
    InvariantPointAttention:
        c_hidden: 16
        N_head: 6 # 12
        N_query_points: 4
        N_point_values: 8