---

# wandb settings
project_name: flow_matching
model_name: all_atom_de_novo

# training settings
trainer:
    devices: 1
    max_epochs: 200
    val_check_interval: 0.5
    gradient_clip_val: 1.0
checkpoint:
    path: null
    load_optimizer: False

# abflow datamodule settings
datamodule:
    num_workers: 8
    batch_size: 2 # batch size per GPU
    dataset:
        name: sabdab
        seed: 2025
        load_val_test: True  # True only during training, and False during data pre-processing
        scheme: chothia       # Scheme to use for sequence alignment. Default: chothia
        num_val_cluster: 10
        paths:
            data: /scratch/hz362/datavol/data/
            model: /scratch/hz362/datavol/model/
        redesign:
            framework: False
            hcdr1: False
            hcdr2: False
            hcdr3: True
            lcdr1: True
            lcdr2: False
            lcdr3: False
        crop: 
            max_crop_size: 180 # ideally 192
            antigen_crop_size: 60 # ideally 64

# abflow model settings
model:
    loss_weighting:
        trans_vf_loss: 1.0
        rots_vf_loss: 10.0
        seq_vf_loss: 100.0
        plddt_loss: 0.01
        distogram_loss: 0.01
    learning_rate: 0.0001
    design_mode: ["sequence", "backbone", "sidechain"]
    seed: 42

# abflow network settings
network:
    design_mode: ["sequence", "backbone", "sidechain"]
    c_s: 96 # default: 384
    c_z: 32 # default: 128
    n_condition_module_blocks: 8 # default: 48
    n_denoising_module_blocks: 4 # default: 24
    n_confidence_head_blocks: 1 # default: 4
    n_cycle: 4
    mini_rollout_steps: 20
    num_time_steps: 500
    self_condition_rate: 0.0
    self_condition_steps: 0
    label_smoothing: 0.0




# additional parameters - consider removing or adding them
network_params: # hidden dim is c_hidden * N_head. The principle is to keep hidden dim same as feature dim c_s or c_z
    Pairformer:
        TriangleMultiplicationOutgoing:
            c_hidden: 32 # 128
        TriangleMultiplicationIncoming:
            c_hidden: 32 # 128
        TriangleAttentionStartingNode:
            c_hidden: 8 # 32
            N_head: 4
        TriangleAttentionEndingNode:
            c_hidden: 8 # 32
            N_head: 4
        AttentionPairBias:
            c_hidden: 16 # 24
            N_head: 6 # 16
    InvariantPointAttention:
        c_hidden: 16
        N_head: 6 # 12
        N_query_points: 4
        N_point_values: 8